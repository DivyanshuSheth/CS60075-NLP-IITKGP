{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5_18QE30008.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Assignment 5 on Natural Language Processing\n",
        "\n",
        "## Date : 3rd Nov, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to explore various language models specifically LSTM based and transformer. We will explore how the size of the model effects the sequence generated. We will see both character based and word based models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      },
      "source": [
        "Please submit with outputs. Submissions without predicted outputs will be penalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXdkhxZAXnTW"
      },
      "source": [
        "# Word Based LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbU5DRolXseI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7bbaa47-5f1c-4454-be1d-a3583d6a9736"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Embedding\n",
        "from keras.utils import np_utils\n",
        "import numpy\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2NR3RFFYOT8"
      },
      "source": [
        "Do basic pre processing which includes lowering etc\n",
        "Check the dataset and apply suitable preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQvfF2NjXxGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a737bd1-ef4a-4be7-8fb2-8f6914a1fe42"
      },
      "source": [
        "# Load the data and preprocess data and store corpus in raw_text\n",
        "!rm -r sample_data \n",
        "!wget 'https://www.dropbox.com/s/ogdasj2a375719a/corpus.txt?dl=1'\n",
        "os.rename('./corpus.txt?dl=1', 'corpus.txt')\n",
        "file = open(\"corpus.txt\", \"r\")\n",
        "original_text = file.read()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n",
            "--2020-11-17 15:21:20--  https://www.dropbox.com/s/ogdasj2a375719a/corpus.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6023:1::a27d:4301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/ogdasj2a375719a/corpus.txt [following]\n",
            "--2020-11-17 15:21:20--  https://www.dropbox.com/s/dl/ogdasj2a375719a/corpus.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc11b86d420b346dcf6418b1af18.dl.dropboxusercontent.com/cd/0/get/BDag5O-_9SooJyMCMk7Fyo05R38DjESPOD7d-T3P105BmVgjjJtqtd-zyesPjwKXDioaEd3_vSJYszJ9ihMKQGvleycuIlt5KBTCtd9uL5ofpGZR-Qgcj-PL6dtnpAIejvo/file?dl=1# [following]\n",
            "--2020-11-17 15:21:20--  https://uc11b86d420b346dcf6418b1af18.dl.dropboxusercontent.com/cd/0/get/BDag5O-_9SooJyMCMk7Fyo05R38DjESPOD7d-T3P105BmVgjjJtqtd-zyesPjwKXDioaEd3_vSJYszJ9ihMKQGvleycuIlt5KBTCtd9uL5ofpGZR-Qgcj-PL6dtnpAIejvo/file?dl=1\n",
            "Resolving uc11b86d420b346dcf6418b1af18.dl.dropboxusercontent.com (uc11b86d420b346dcf6418b1af18.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc11b86d420b346dcf6418b1af18.dl.dropboxusercontent.com (uc11b86d420b346dcf6418b1af18.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 148964 (145K) [application/binary]\n",
            "Saving to: ‘corpus.txt?dl=1’\n",
            "\n",
            "corpus.txt?dl=1     100%[===================>] 145.47K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-11-17 15:21:21 (3.86 MB/s) - ‘corpus.txt?dl=1’ saved [148964/148964]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsNPlih8g4w3"
      },
      "source": [
        "def preprocess(raw_text):\n",
        "    raw_text = raw_text.lower().replace('_', '')\n",
        "    # split_text = raw_text.split('\\n')\n",
        "    # for i, line in enumerate(split_text):\n",
        "    #     if line[:7] == 'CHAPTER':\n",
        "    #         del split_text[i]\n",
        "    # raw_text = '\\n'.join(split_text)\n",
        "    wordbased_raw_text = re.sub(r'[^\\w\\s]', '', raw_text)\n",
        "    return wordbased_raw_text, raw_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiVm09gRv2nd"
      },
      "source": [
        "word_raw_text, character_raw_text = preprocess(original_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dux6cK80h_t9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c459c872-e5a3-444f-9e0e-71dbfe8ebed3"
      },
      "source": [
        "words = nltk.word_tokenize(word_raw_text)\n",
        "print(\"No. of unique tokens for the word-based model = {}\".format(len(set(words))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of unique tokens for the word-based model = 2751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eug68GOecM8Z"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = 2751 # choose based on statistics\n",
        "oov_tok = '<OOV>'\n",
        "embedding_dim = 100\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNBOlJ5cQym"
      },
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts([word_raw_text])\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNRJDbFcdHbO"
      },
      "source": [
        "seq_length = 50\n",
        "tokens = tokenizer.texts_to_sequences([word_raw_text])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykfI4FrwdyJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247b356b-7914-4276-94ac-b625331d8c7b"
      },
      "source": [
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, len(tokens) - seq_length-1 , 1):\n",
        "  seq_in = tokens[i:i + seq_length]\n",
        "  seq_out = tokens[i + seq_length]\n",
        "\n",
        "  if seq_out==1: #Skip samples where target word is OOV\n",
        "    continue\n",
        "    \n",
        "  dataX.append(seq_in)\n",
        "  dataY.append(seq_out)\n",
        " \n",
        "N = len(dataX)\n",
        "print (\"Total training data size: \", N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training data size:  26326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJmGr1xId8cO"
      },
      "source": [
        "X = numpy.array(dataX)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = numpy.array(dataY)\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QPApRA-d9JV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "583f40e5-8609-4688-83b0-5d3bb11aabce"
      },
      "source": [
        "# with embedding\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 100)           275100    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2751)              354879    \n",
            "=================================================================\n",
            "Total params: 714,459\n",
            "Trainable params: 714,459\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14ClAAYpeCVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e564e6bb-9ed4-4a3b-f6c5-fbb6f64cf3cf"
      },
      "source": [
        "# Use validation split of 0.2 while training\n",
        "model.fit(X, y, epochs=12, batch_size=128, validation_split=0.2) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "165/165 [==============================] - 7s 44ms/step - loss: 6.4338 - accuracy: 0.0544 - val_loss: 6.2618 - val_accuracy: 0.0849\n",
            "Epoch 2/12\n",
            "165/165 [==============================] - 7s 39ms/step - loss: 5.9567 - accuracy: 0.0561 - val_loss: 6.1848 - val_accuracy: 0.0849\n",
            "Epoch 3/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 5.8237 - accuracy: 0.0619 - val_loss: 6.1638 - val_accuracy: 0.0934\n",
            "Epoch 4/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 5.7180 - accuracy: 0.0684 - val_loss: 6.1695 - val_accuracy: 0.0946\n",
            "Epoch 5/12\n",
            "165/165 [==============================] - 7s 39ms/step - loss: 5.6258 - accuracy: 0.0756 - val_loss: 6.1667 - val_accuracy: 0.0929\n",
            "Epoch 6/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 5.5251 - accuracy: 0.0843 - val_loss: 6.1441 - val_accuracy: 0.0949\n",
            "Epoch 7/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 5.4204 - accuracy: 0.0943 - val_loss: 6.1232 - val_accuracy: 0.1001\n",
            "Epoch 8/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 5.3120 - accuracy: 0.1022 - val_loss: 6.0996 - val_accuracy: 0.1043\n",
            "Epoch 9/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 5.2038 - accuracy: 0.1076 - val_loss: 6.0803 - val_accuracy: 0.1092\n",
            "Epoch 10/12\n",
            "165/165 [==============================] - 7s 39ms/step - loss: 5.0972 - accuracy: 0.1203 - val_loss: 6.0876 - val_accuracy: 0.1105\n",
            "Epoch 11/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 4.9964 - accuracy: 0.1268 - val_loss: 6.0920 - val_accuracy: 0.1071\n",
            "Epoch 12/12\n",
            "165/165 [==============================] - 7s 40ms/step - loss: 4.8995 - accuracy: 0.1351 - val_loss: 6.0840 - val_accuracy: 0.1147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6a57d117b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg9WSEwYeMAH"
      },
      "source": [
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items())) ## Create word to idx map using tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mhL0J0eQku"
      },
      "source": [
        "# Complete the code to return next n words greedily\n",
        "def next_tokens(input_str, n): \n",
        "\t\tprint (\"Seed: \\n\",  input_str)\n",
        "\t\tresult_string = ''\n",
        "\t\tfor i in range(n):\n",
        "\t\t\t\tinput_sequence = tokenizer.texts_to_sequences([input_str])[0]\n",
        "\t\t\t\tprediction = model.predict(input_sequence, verbose=0)\n",
        "\t\t\t\tpredicted_word = reverse_word_map[numpy.argmax(prediction[0])]\n",
        "\t\t\t\tresult_string = result_string + predicted_word + ' ' \n",
        "\t\t\t\tinput_str = input_str + ' ' + predicted_word\n",
        "\t\t\t\tinput_str = ' '.join(input_str.split(' ')[1:])\n",
        "\t\treturn result_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ_NTQezeWYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad6f6c3-9cd3-4cd4-c16b-bab021ee6f26"
      },
      "source": [
        "# pick a random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "input_str = ' '.join([reverse_word_map[value] for value in pattern])\n",
        "\n",
        "print('\\nGenerated: \\n' + next_tokens(input_str , 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \n",
            " hare he denies it said the king leave out that part well at any rate the dormouse said the hatter went on looking anxiously round to see if he would deny it too but the dormouse denied nothing being fast asleep after that continued the hatter i cut some more\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"embedding_2_input:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "\n",
            "Generated: \n",
            "the know said was the queen alice realitythe duchess the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-x8W-OSVgNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf177dc-ceac-4fd5-be85-e4732c987d74"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "\n",
        "# Use first 50 tokens from given input_str as input.(Use tokenizer to split to take first 50)\n",
        "print('\\nGenerated: \\n' + next_tokens(input_str, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \n",
            " The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not  a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
            "\n",
            "Generated: \n",
            "queen not duchess the queen little know little the voice \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_R5Tngo3_D"
      },
      "source": [
        "# Character based LSTM Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zZaHsejo57p"
      },
      "source": [
        "# User the preprocess data and create raw_text\n",
        "raw_text = character_raw_text\n",
        "# create mapping of unique characters to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkVVDbump0Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e92323-ac0b-419e-9bb1-5e48b0f7ad63"
      },
      "source": [
        "# Print the total characters and character vacob size\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "\n",
        "print('Total number of characters = {}\\nVocab size = {}'.format(n_chars, n_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters = 142037\n",
            "Vocab size = 45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aVserymqE1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d680ada3-dff3-45be-b938-7a9c65f7833f"
      },
      "source": [
        "'''\n",
        "Prepare dataset where the input is sequence of 100 characters and target is next character.\n",
        "'''\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\n",
        "    input_seq = raw_text[i: i + seq_length]\n",
        "    output_seq = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in input_seq])\n",
        "    dataY.append(char_to_int[output_seq])\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  141937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic4yf4hNqc7T"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# one hot encode the output variable\n",
        "dataY = numpy.array(dataY)\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvawnjFVqhMi"
      },
      "source": [
        "embedding_dim =100\n",
        "max_length =100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek5DqNeTqkAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a05c844-fd99-4e66-a876-9884046c37bc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 100, 100)          4500      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               365568    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 45)                11565     \n",
            "=================================================================\n",
            "Total params: 381,633\n",
            "Trainable params: 381,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFoStJIOqpM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90451553-599b-448a-b05f-f186b3eaf646"
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 2.2060\n",
            "Epoch 2/20\n",
            "1109/1109 [==============================] - 46s 41ms/step - loss: 1.7016\n",
            "Epoch 3/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.5040\n",
            "Epoch 4/20\n",
            "1109/1109 [==============================] - 47s 42ms/step - loss: 1.3830\n",
            "Epoch 5/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.3012\n",
            "Epoch 6/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.2383\n",
            "Epoch 7/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.1870\n",
            "Epoch 8/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.1443\n",
            "Epoch 9/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.1045\n",
            "Epoch 10/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.0692\n",
            "Epoch 11/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.0369\n",
            "Epoch 12/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 1.0069\n",
            "Epoch 13/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 0.9774\n",
            "Epoch 14/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 0.9524\n",
            "Epoch 15/20\n",
            "1109/1109 [==============================] - 47s 42ms/step - loss: 0.9292\n",
            "Epoch 16/20\n",
            "1109/1109 [==============================] - 47s 42ms/step - loss: 0.9064\n",
            "Epoch 17/20\n",
            "1109/1109 [==============================] - 47s 42ms/step - loss: 0.8865\n",
            "Epoch 18/20\n",
            "1109/1109 [==============================] - 47s 42ms/step - loss: 0.8685\n",
            "Epoch 19/20\n",
            "1109/1109 [==============================] - 47s 42ms/step - loss: 0.8526\n",
            "Epoch 20/20\n",
            "1109/1109 [==============================] - 46s 42ms/step - loss: 0.8373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6a5764ab70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mne5uG1cNAV9"
      },
      "source": [
        "#Get the generated string using the model.\n",
        "def predict_next_100_chars(pattern, n):\n",
        "    for i in range(n):\n",
        "      reshaped = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "      prediction = model.predict(reshaped, verbose=0)\n",
        "      print (int_to_char[numpy.argmax(prediction)], end = '')  \n",
        "      # seq_in = [int_to_char[value] for value in pattern]\n",
        "      pattern.append(numpy.argmax(prediction))\n",
        "      pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIR5hzQFNByu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dc43d3-4ef1-41fd-bc29-f4e73aae54be"
      },
      "source": [
        "#pick a random seed\n",
        "np.random.seed(40)\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "input_str = ''.join([int_to_char[value] for value in pattern])\n",
        "print(\"Seed:\\n\" + input_str + '\\n')\n",
        "print(\"\\nGenerated string:\\n\")\n",
        "predict_next_100_chars(pattern, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "e often seen them at dinn—” she checked herself hastily.\n",
            "“i don’t know where dinn may be,” said the dodo\n",
            "\n",
            "\n",
            "Generated string:\n",
            "\n",
            " to the caterpillar and alice foldow it was the first to stair, and she was so much as she could not like the same thing as it was a little shriek, and was so much and see what was the first to stair,"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iutpuJAgrgU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce83c0cf-61a5-4902-99be-eff5e5c04e60"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "#Use the first 100 characters from given input_str as input and generate next 200 characters. \n",
        "input_str = input_str.lower()\n",
        "input_string = ''\n",
        "for each in input_str:\n",
        "  if each in chars:\n",
        "    if (len (input_string)<100):\n",
        "      input_string += each\n",
        "pattern = []\n",
        "pattern.append([char_to_int[char] for char in input_string])\n",
        "print (\"\\nGenerated:\\n\")\n",
        "predict_next_n_chars(pattern[0], 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Generated:\n",
            "\n",
            "nd the mock turtle said to herself “the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8shbcukr0tJ"
      },
      "source": [
        "## Character based LSTM Model 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWBPCrTdr46U"
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
        "model1.add(LSTM(256, input_shape=(X.shape[1], embedding_dim),return_sequences=True))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(256))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(Dense(y.shape[1], activation='softmax'))\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZxrtjFIr63L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3855e8db-7c32-4342-ec28-2a0a962f106d"
      },
      "source": [
        "model1.fit(X, y, epochs=20, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2218/2218 [==============================] - 148s 67ms/step - loss: 2.0989\n",
            "Epoch 2/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 1.5660\n",
            "Epoch 3/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 1.3901\n",
            "Epoch 4/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 1.2894\n",
            "Epoch 5/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 1.2168\n",
            "Epoch 6/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 1.1630\n",
            "Epoch 7/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 1.1186\n",
            "Epoch 8/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 1.0812\n",
            "Epoch 9/20\n",
            "2218/2218 [==============================] - 146s 66ms/step - loss: 1.0471\n",
            "Epoch 10/20\n",
            "2218/2218 [==============================] - 146s 66ms/step - loss: 1.0167\n",
            "Epoch 11/20\n",
            "2218/2218 [==============================] - 146s 66ms/step - loss: 0.9952\n",
            "Epoch 12/20\n",
            "2218/2218 [==============================] - 146s 66ms/step - loss: 0.9693\n",
            "Epoch 13/20\n",
            "2218/2218 [==============================] - 146s 66ms/step - loss: 0.9500\n",
            "Epoch 14/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 0.9311\n",
            "Epoch 15/20\n",
            "2218/2218 [==============================] - 146s 66ms/step - loss: 0.9167\n",
            "Epoch 16/20\n",
            "2218/2218 [==============================] - 146s 66ms/step - loss: 0.9029\n",
            "Epoch 17/20\n",
            "2218/2218 [==============================] - 147s 66ms/step - loss: 0.8910\n",
            "Epoch 18/20\n",
            "2218/2218 [==============================] - 148s 67ms/step - loss: 0.8790\n",
            "Epoch 19/20\n",
            "2218/2218 [==============================] - 148s 67ms/step - loss: 0.8709\n",
            "Epoch 20/20\n",
            "2218/2218 [==============================] - 148s 67ms/step - loss: 0.8609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f69f7440ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtPq205gOJd3"
      },
      "source": [
        "# Generate the sequence similar to above methods. Get the generated string using the model.\n",
        "def predict_next_n_chars(pattern, n):\n",
        "    for i in range(n):\n",
        "      x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "      prediction = model1.predict(x, verbose=0)\n",
        "      print (int_to_char[numpy.argmax(prediction)], end = '') \n",
        "      seq_in = [int_to_char[value] for value in pattern]\n",
        "      pattern.append(numpy.argmax(prediction))\n",
        "      pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXRgU2q-OMb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c629da2f-6058-4a70-efa0-7ecd15b6c162"
      },
      "source": [
        "#pick a random seed\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "input_str = ''.join([int_to_char[value] for value in pattern])\n",
        "print (\"Seed:\",  input_str, sep = '\\n\\n')\n",
        "print (\"\\nGenerated:\\n\")\n",
        "predict_next_n_chars(pattern, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\n",
            "looking at the house, and wondering what to do next, when suddenly a footman in livery came running o\n",
            "\n",
            "Generated:\n",
            "\n",
            "n the subject.\n",
            "“what a party was a little sisters of the court,” and she was a little shriek of the words “drink me,” said the king.\n",
            "“i can go on the sea,” said the mock turtle.\n",
            "“come on, i should thi"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sVtI3zCON7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78732a75-0fe1-4ded-c24c-5b598cdd3fb9"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "#Use the first 100 characters from given input_str as input and generate next 200 characters. \n",
        "input_str = input_str.lower()\n",
        "input_string = ''\n",
        "for each in input_str:\n",
        "  if each in chars:\n",
        "    if (len (input_string)<100):\n",
        "      input_string += each\n",
        "pattern = []\n",
        "pattern.append([char_to_int[char] for char in input_string])\n",
        "print (\"\\nGenerated:\\n\")\n",
        "predict_next_n_chars(pattern[0], 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Generated:\n",
            "\n",
            "nd the mock turtle said to herself “the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral of that is—‘the moral "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPrjxjoNsaQC"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW5_IlBeZAPq"
      },
      "source": [
        "**Question:** What are your observations based on the model(all) outputs on train data(in domain) vs unseen data(out of domain) ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YryzP1upZv5h"
      },
      "source": [
        "**Answer:**\n",
        "The models do not perform well on out of domain data. The performance on in-domain data is comparatively better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBzCD0I0Z3uP"
      },
      "source": [
        "**Question:** What did you observe in the outputs of char LSTM model1 vs char LSTM model2 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUHxHmXZaNdn"
      },
      "source": [
        "**Answer:**\n",
        "Model 2, being more sophisticated in nature, is expected to perform better than Model 1 at coherence and grammar in the output sentence. Both models do not perform well, however."
      ]
    }
  ]
}